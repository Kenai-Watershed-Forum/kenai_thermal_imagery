# Water Temperature Loggers

---
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

```{r , include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
# clear environment
rm(list=ls())

# load packages
library(bookdown)
library(tidyverse)
library(googlesheets4)
library(lubridate)
library(readr)
library(readxl)
library(writexl)
library(hms)
library(plotly)
library(DT)
library(xlsx)
library(leaflet)
library(DT)
library(ggpubr)
library(ggpmisc)
library(plotrix)
library(packrat)
library(foreign)

# set plotting themes

## geom_col plots theme
col_theme <- theme(axis.title = element_text(size = 14, face = "bold"),
                   strip.text = element_text(size = 14, face = "bold"),
                   legend.title = element_text(size = 14, face = "bold"),
                   legend.text = element_text(size = 14),
                   axis.text = element_text(size = 14))

## geom_points plots theme
points_theme <- theme(axis.title = element_text(size = 14, face = "bold"),
                   strip.text = element_text(size = 14, face = "bold"),
                   legend.title = element_text(size = 14, face = "bold"),
                   legend.text = element_text(size = 14),
                   axis.text = element_text(size = 11, face = "bold"),
                   title = element_text(size = 18))

# function to exclude multiple items per column
'%ni%' <- Negate('%in%')

# clarify select function
select <- dplyr::select
```

## Introduction

In addition to water temperature data collected via airborne thermal infrared imagery on July 20, 2020, we deployed water temperature loggers in the lower reach of each study stream in order to collect continuous time series of water temperature.

This section executes methods to visualize and perform basic quality checks on continuous temperature loggers deployed in the study streams.

We used Onset HOBO Temp Pro V2 loggers, deployed and maintained according to standards published in [Mauger et al. 2015](https://www.sciencedirect.com/science/article/pii/S2214581815000932 "link").

```{r, echo = F}

# general intention: if mainistem is very warm, value of off-channel seeps all the more valuable

```




<br>

## Map

```{r, echo = F}
# general prep
## read in all site coordinates
coords <- read.csv("input/temperature_logger_data/site_metadata.csv") 
```



```{r, echo = F}
## read in logger data

# beaver creek
# we have logger data both from the KWF site and the nearby (~200 m upstream) UAA site

## read in KWF files
dir <- "input/temperature_logger_data/beaver_creek/csv_files/kwf/"
kwf_beaver_files <- list.files(dir)

kwf_beaver_creek <- list.files(path = dir,
              pattern="*.csv", 
              full.names = T) %>% 
    map_df(~read_csv(.)) %>%
  # assign new columns
  mutate(site = "kwf_beaver_creek",
         org = "KWF") %>%
  mutate(date_time = mdy_hms(paste(date, time))) %>%
  select(-date,-time) 

## read in UAA files
dir <- "input/temperature_logger_data/beaver_creek/csv_files/uaa/"
uaa_beaver_files <- list.files(dir)

uaa_beaver_creek <- list.files(path = dir,
              pattern="*.csv", 
              full.names = T) %>% 
    map_df(~read_csv(.)) %>%
  # correct format to match kwf files
  select(date_time,temp_C,logger_id) %>%
  transform(date_time = mdy_hm(date_time)) %>%
  # assign new columns
  mutate(site = "uaa_beaver_creek",
         org = "UAA") 
  
# combine UAA and KWF data to single dataframe, prep columns
beaver_creek <- bind_rows(kwf_beaver_creek,uaa_beaver_creek) 


# ******************************

# funny river

dir <- "input/temperature_logger_data/funny_river/csv_files"
funny_files <- list.files(dir)

funny_river <- list.files(path = dir,
              pattern="*.csv", 
              full.names = T) %>% 
    map_df(~read_csv(.)) %>%
  # assign new columns
  mutate(site = "funny_river",
         org = "KWF") %>%
  transform(date_time = mdy_hm(date_time))


# ******************************

# moose river

dir <- "input/temperature_logger_data/moose_river/csv_files"
moose_files <- list.files(dir)

moose_river <- list.files(path = dir,
              pattern="*.csv", 
              full.names = T) %>% 
    map_df(~read_csv(.)) %>%
  # assign new columns
  mutate(site = "moose_river",
         org = "KWF") %>%
  transform(date_time = mdy_hm(date_time))


# ******************************

# lower crooked creek

dir <- "input/temperature_logger_data/lower_crooked_creek/csv_files"
lower_crooked_files <- list.files(dir)

lower_crooked_creek <- list.files(path = dir,
              pattern="*.csv", 
              full.names = T) %>% 
    map_df(~read_csv(.)) %>%
  # assign new columns
  mutate(site = "lower_crooked_creek",
         org = "KWF") %>%
  transform(date_time = mdy_hm(date_time))


# ******************************
  
# lower crooked creek - real time logger (Beaded Stream)
# https://inletkeeper.org/our-work/healthy-habitat/real-time-temperature-sites/crooked-creek/
# download latest version of this file from inletkeeper site before republishing this document

# 9/10/21 - current file from website has software issue; reads as 2009 instead of 2020 after the observation on "2019-12-31 15:00:00". Contacted 9/10/21. Response: this is a software bug; requires site visit to correct. For now, correct by adding +10 yrs after erroneous date

# file location: 

erroneous_time <- as.POSIXct("2019-12-31 15:00:00",origin = "1970-01-01")

dir <- "input/real_time_temp_sensor/"

lower_crooked_realtime_file <- as.character(list.files(dir))

lower_crooked_creek_realtime <- read_csv(paste0(dir,lower_crooked_realtime_file), skip = 3) %>%
  # assign new columns
  mutate(site = "lower_crooked_creek_realtime",
         org = "CIK",
         logger_id = "realtime") %>%
  filter(!is.na("Water Temp")) %>%
    # rename columns to match others
  rename("temp_C" = "Water Temp",
         "date_time" = "Record Time (AKDT)") %>%
  select(site,org,logger_id,temp_C,date_time) %>%

  # prep columns for +10 yrs time correction
  transform(date_time = ymd_hms(date_time)) %>%
  mutate(year = year(date_time),
         month = month(date_time),
         daymonth = format(as.Date(date_time), "%d"),
         time = as_hms(date_time)) %>%

  # apply time error correction described above (+10 yrs after "2019-12-31 15:00:00")
  mutate(row = row_number(),
         year = ifelse(row < 9725, year + 10, year),
         # reassemble date_time
         date_time = ymd_hms(paste0(year,"-",month,"-",daymonth," ",time))) %>%
  
  # remove erroneous year 2010 values
  filter(year != "2010") %>%
  select(-month,-daymonth,-time,-row,-year) 
  
  

# ******************************

# upper crooked creek

dir <- "input/temperature_logger_data/upper_crooked_creek/csv_files"
upper_crooked_files <- list.files(dir)

upper_crooked_creek <- list.files(path = dir,
              pattern="*.csv", 
              full.names = T) %>% 
    map_df(~read_csv(.)) %>%
  # assign new columns
  mutate(site = "upper_crooked_creek",
         org = "KWF") %>%
  transform(date_time = mdy_hm(date_time))


# ******************************
# ******************************
# ******************************

# combine data from all sites into single dataframe
dat <- bind_rows(beaver_creek,funny_river,moose_river,lower_crooked_creek,
                 upper_crooked_creek) %>%
  transform(logger_id = as.character(logger_id)) %>%
  bind_rows(lower_crooked_creek_realtime) %>%
  
  mutate(day = yday(date_time),
         year = year(date_time)) %>%
  distinct()

```



```{r, echo = F}

# Exclude segments of time series where loggers are exposed (pre/post deployment or low water)

# what are all our unique loggers?
(loggers <- as.data.frame(unique(dat$logger_id)))


# create ggplotly chart for each time series, one at a time, by remove hashtag from logger id

## double-hashtag indicates that visual inspection was performed and flagged data identified in "input/temperature_logger_data/flagged_data.csv". Single hashtag indicates that visual inspection has not yet been performed

 logger <- "20861017"

## logger <- "20861019"

## logger <- "20861209"

## logger <- "20861215"

## logger <- "10816958"

## logger <- "20635544"

## logger <- "20861027"

##logger <- "20861028"

## logger <- "20012607"

## logger <- "20012612"

## logger <- "20861024"

## logger <- "20861029"

## logger <- "10816959"

## logger <- "20012598"

## logger <- "20861018"

## logger <- "20861022"

## logger <- "20861016"

## logger <- "20861023"

## logger <- "realtime"



# plot
ggplotly(
  p <- dat %>%
  # modified site one at a time here to visually inspect datasets
  filter(logger_id == logger
         ) %>%
  
  ggplot(aes(date_time,temp_C, label = day)) +
  geom_point() +
  ggtitle(logger)
  )

```





Exclude flagged data
```{r}
# read in file of visualy identified flagged data
flagged_data <- read.csv("input/temperature_logger_data/flagged_data.csv") %>%
  select(-notes)

# apply useData = 0 flags to flagged data
flagged_data <- inner_join(dat,flagged_data) %>%
  filter(day >= day_start & day <= day_stop) %>%
  mutate(useData = 0) %>%
  select(-day_start,-day_stop)

# apply useData = 1 to non-flagged data
nonflagged_data <- anti_join(dat,flagged_data) %>%
  mutate(useData = 1)

# rejoin flagged and non-flagged data in same dataframe
dat <- bind_rows(flagged_data,nonflagged_data)
  


```


```{r, echo = F}
# create ggplotly chart with flagged data removed
ggplotly(
  p <- dat %>%
  # modified site one at a time here to visually inspect datasets
  filter(logger_id == logger,
         useData == 1
         ) %>%
  
  ggplot(aes(date_time,temp_C, label = day)) +
  geom_point() +
  ggtitle(logger)
  )

```










```{r}
 # ***Rectifying missing data***

# Loggers at the Beaver Creek logger site were deployed in early summer 2020 by KWF. As part of a separate research project, UAA established a logger site several hundred meters upstream.

# We want to examine if data from the two sites are similar enough to use as a proxy when data observations are missing.


bc_dat <- dat %>%
  filter(site %in% c("kwf_beaver_creek","uaa_beaver_creek"),
         useData == 1) %>%
  group_by(site,date_time) %>%
  summarise(temp_C = mean(temp_C)) %>%
  pivot_wider(names_from = site, values_from = temp_C) %>%
  filter(!is.na(uaa_beaver_creek))

bc_dat %>%
  ggplot(aes(kwf_beaver_creek,uaa_beaver_creek)) +
  geom_point() +
  ylim(9,18) +
  geom_smooth(method = 'lm') +
  stat_poly_eq(formula = y ~ x, 
                aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
                parse = TRUE) +
  labs(title = paste("Number of paired observations: ", nrow(bc_dat)))

```

What is the average difference in temperature between the two logger sites?
```{r}
bc_diff <- bc_dat %>%
  mutate(diff = abs(kwf_beaver_creek - uaa_beaver_creek)) %>%
  filter(!is.na(diff)) %>%
  summarise(avg_diff = mean(diff))
```

Average difference (absolute value) is `r bc_diff` C

